# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import mean_absolute_error

# 1. Loading and Preprocessing

#Markdown
### Data Preprocessing Steps

1. **Missing Value Check**:  
   Verified the dataset for missing values. No missing data was found, so no imputation was necessary.

2. **Feature and Target Separation**:  
   Separated the input features (`X`) and target variable (`y`) to prepare for model training.

3. **Feature Scaling (Standardization)**:  
   Applied `StandardScaler` to scale all features to a mean of 0 and standard deviation of 1.  
   This step is especially important for models like **SVR** and **Linear Regression**, which are sensitive to feature magnitudes.

4. **Train-Test Split**:  
   Split the dataset into **80% training** and **20% testing** data using `train_test_split`, ensuring unbiased model evaluation.

# Load dataset
housing = fetch_california_housing()
df = pd.DataFrame(housing.data, columns=housing.feature_names)
df['MedHouseVal'] = housing.target
print(df.head())

# Check for missing values
print(df.isnull().sum())

# There are no missing values. Now, perform feature scaling
features = df.drop('MedHouseVal', axis=1)
target = df['MedHouseVal']

# Standardize the features
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)

#2. Regression Algorithm Implementation
#Markdown
### 2.1 Linear Regression

**How it works:**  
Linear Regression finds the best-fitting linear relationship between the input features and the target variable by minimizing the mean squared error.

**Why it's suitable:**  
It's a good baseline model for comparison and helps identify linear relationships in the data.

#code
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)

print("Linear Regression:")
print("MSE:", mean_squared_error(y_test, y_pred_lr))
print("R² Score:", r2_score(y_test, y_pred_lr))

#Markdown
## 2.2 Decision Tree Regressor

**How it works:**  
Splits the dataset into branches using feature thresholds, forming a tree. The final prediction is the average value in each leaf.

**Why suitable?**  
Handles non-linear relationships well and doesn’t require feature scaling.

#code
dt = DecisionTreeRegressor(random_state=42)
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

print("Decision Tree Regressor:")
print("MSE:", mean_squared_error(y_test, y_pred_dt))
print("R² Score:", r2_score(y_test, y_pred_dt))

#Markdown
## 2.3 Random Forest Regressor

**How it works:**  
An ensemble of decision trees. It trains multiple trees on random subsets and averages the predictions.

**Why suitable?**  
Reduces overfitting, improves accuracy, and handles large feature sets well.

#code
rf = RandomForestRegressor(random_state=42, n_estimators=100)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

print("Random Forest Regressor:")
print("MSE:", mean_squared_error(y_test, y_pred_rf))
print("R² Score:", r2_score(y_test, y_pred_rf))

#Markdown
### 2.4 Gradient Boosting Regressor

**How it works:**

Gradient Boosting is an ensemble method that builds models sequentially, where each new model tries to correct the errors made by the previous ones. It combines the predictions of many weak learners (typically decision trees) to produce a strong model. At each step, it fits a new model to the **residual errors** of the previous model using gradient descent to minimize a loss function.

**Why it's suitable:**

- Captures **complex, non-linear relationships**.
- Often delivers **high accuracy** on structured/tabular datasets like California Housing.
- Works well with relatively smaller data if properly tuned.
- Robust to outliers and can handle interactions between variables effectively.

#code
gb = GradientBoostingRegressor(random_state=42, n_estimators=100)
gb.fit(X_train, y_train)
y_pred_gb = gb.predict(X_test)

print("Gradient Boosting Regressor:")
print("MSE:", mean_squared_error(y_test, y_pred_gb))
print("R² Score:", r2_score(y_test, y_pred_gb))

#Markdown
### 2.5 Support Vector Regressor (SVR)

**How it works:**

Support Vector Regression (SVR) extends Support Vector Machines (SVMs) for regression tasks. Instead of trying to classify data points, SVR tries to fit the best possible line (or curve) within a margin of tolerance (epsilon) around the actual data points. It uses **kernel functions** (like linear, polynomial, or RBF) to model **non-linear relationships** in the data.

**Why it's suitable:**

- SVR is effective for datasets where the **relationship between features and target is non-linear**.
- It performs well in **high-dimensional spaces**.
- It's useful when we want a **robust regression model** that ignores small deviations/errors.
- Feature scaling is essential for SVR, which we've already handled in preprocessing.

#code
svr = SVR(kernel='rbf')
svr.fit(X_train, y_train)
y_pred_svr = svr.predict(X_test)

print("Support Vector Regressor:")
print("MSE:", mean_squared_error(y_test, y_pred_svr))
print("R² Score:", r2_score(y_test, y_pred_svr))

# 3. Model Evaluation and Comparison

def evaluate(y_test, y_pred):
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    return mse, mae, r2

results = {
    'Linear Regression': evaluate(y_test, y_pred_lr),
    'Decision Tree': evaluate(y_test, y_pred_dt),
    'Random Forest': evaluate(y_test, y_pred_rf),
    'Gradient Boosting': evaluate(y_test, y_pred_gb),
    'SVR': evaluate(y_test, y_pred_svr)
}

# Convert to DataFrame
results_df = pd.DataFrame(results, index=['MSE', 'MAE', 'R2']).T
print(results_df)

print("\nBest model based on R2 Score:")
print(results_df.sort_values(by='R2', ascending=False).head(1))

print("\nWorst model based on R2 Score:")
print(results_df.sort_values(by='R2').head(1))

#Markdown
## Model Comparison and Conclusion

### Best Performing Model:
**Gradient Boosting Regressor** showed the best performance in terms of **R² Score** and also maintained relatively low **MSE** and **MAE**. This is because:
- It handles non-linear relationships effectively.
- It iteratively improves accuracy by minimizing errors.

### Worst Performing Model:
**Decision Tree Regressor** had the lowest performance. Although it handles non-linearity, it tends to **overfit** the training data, leading to **poor generalization** on test data.

### Summary Table:
Refer to the evaluation summary table above for detailed metrics comparison across all models.




